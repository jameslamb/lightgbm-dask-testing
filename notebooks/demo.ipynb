{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noticed-account",
   "metadata": {},
   "source": [
    "# LightGBM + Dask\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"./_img/lightgbm.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./_img/dask-horizontal.svg\" width=\"300\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "This notebook shows how to use `lightgbm.dask` to train a LightGBM model on data stored as a [Dask Array](https://docs.dask.org/en/latest/array.html).\n",
    "\n",
    "To explore other topics in greater depth, see the other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-incentive",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Set up a local Dask cluster\n",
    "\n",
    "Create a cluster with 3 workers. Since this is a `LocalCluster`, those workers are just 3 local processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dietary-multimedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the dashboard: http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/jovyan/testing/notebooks/dask-worker-space/worker-4a06ny6l', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/jovyan/testing/notebooks/dask-worker-space/worker-jdh8k3nf', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/jovyan/testing/notebooks/dask-worker-space/worker-nvz3kbks', purging\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = LocalCluster(n_workers=n_workers)\n",
    "\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(n_workers)\n",
    "\n",
    "print(f\"View the dashboard: {cluster.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-paper",
   "metadata": {},
   "source": [
    "Click the link above to view a diagnostic dashboard while you run the training code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-recommendation",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Get some training data\n",
    "\n",
    "This example uses `sklearn.datasets.make_regression()` to generate a dataset in `numpy` format, then uses `dask.Array.from_array()` to turn that into a Dask Array.\n",
    "\n",
    "That's just done for convenience. `lightgbm.dask` just expects that your data are Dask Arrays or Dask DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "billion-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=10000, random_state=42)\n",
    "dX = da.from_array(X, chunks=(1000, X.shape[1]))\n",
    "dy = da.from_array(y, chunks=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-terrace",
   "metadata": {},
   "source": [
    "Right now, the Dask Arrays `data` and `labels` are lazy. Before training, you can force the cluster to compute them by running `.persist()` and then wait for that computation to finish by `wait()`-ing on them.\n",
    "\n",
    "Doing this is optional, but it will make data loading a one-time cost so subsequent runs are fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "metallic-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "\n",
    "dX = dX.persist()\n",
    "dy = dy.persist()\n",
    "_ = wait([dX, dy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-central",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Train a model\n",
    "\n",
    "With the data set up on the workers, train a model. `lightgbm.dask.DaskLGBMRegressor` has an interface that tries to stay as close as possible to the non-Dask scikit-learn interface to LightGBM (`lightgbm.sklearn.LGBMRegressor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "animated-magnitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/lightgbm/dask.py:506: UserWarning: Parameter n_jobs will be ignored.\n",
      "  _log_warning(f\"Parameter {param_alias} will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding random open ports for workers\n",
      "[LightGBM] [Info] Trying to bind port 59323...\n",
      "[LightGBM] [Info] Binding port 59323 succeeded\n",
      "[LightGBM] [Warning] Connecting to rank 2 failed, waiting for 200 milliseconds\n",
      "[LightGBM] [Info] Trying to bind port 34709...\n",
      "[LightGBM] [Info] Binding port 34709 succeeded\n",
      "[LightGBM] [Warning] Connecting to rank 2 failed, waiting for 200 milliseconds\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Trying to bind port 40681...\n",
      "[LightGBM] [Info] Binding port 40681 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Connected to rank 1\n",
      "[LightGBM] [Info] Connected to rank 2\n",
      "[LightGBM] [Info] Connected to rank 0\n",
      "[LightGBM] [Info] Connected to rank 1\n",
      "[LightGBM] [Info] Connected to rank 0\n",
      "[LightGBM] [Info] Connected to rank 2\n",
      "[LightGBM] [Info] Local rank: 1, total number of machines: 3\n",
      "[LightGBM] [Info] Local rank: 2, total number of machines: 3\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n",
      "[LightGBM] [Info] Local rank: 0, total number of machines: 3\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DaskLGBMRegressor(client=<Client: 'tcp://127.0.0.1:38809' processes=3 threads=3, memory=1.94 GiB>,\n",
       "                  max_depth=5, min_child_samples=1, num_threads=1,\n",
       "                  objective='regression_l1', time_out=120, tree_learner='data')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm.dask import DaskLGBMRegressor\n",
    "\n",
    "dask_reg = DaskLGBMRegressor(\n",
    "    client=client,\n",
    "    max_depth=5,\n",
    "    objective=\"regression_l1\",\n",
    "    learning_rate=0.1,\n",
    "    tree_learner=\"data\",\n",
    "    n_estimators=100,\n",
    "    min_child_samples=1,\n",
    ")\n",
    "\n",
    "dask_reg.fit(\n",
    "    X=dX,\n",
    "    y=dy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-karen",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Evaluate the model\n",
    "\n",
    "The `.predict()` method takes in a Dask collection and returns a Dask Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "logical-handbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<_predict_part, shape=(10000,), dtype=float32, chunksize=(1000,), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "preds = dask_reg.predict(dX)\n",
    "print(str(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-symposium",
   "metadata": {},
   "source": [
    "Before calculating the mean absolute error (MAE) of these predictions, compute some summary statistics on the target variable. This is necessary to understand what \"good\" values of MAE look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "peaceful-damages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0%: -627.63\n",
      "10.0%: -566.9\n",
      "25.0%: -515.82\n",
      "50.0%: -468.06\n",
      "75.0%: -452.54\n",
      "90.0%: -435.72\n",
      "99.0%: -419.1\n"
     ]
    }
   ],
   "source": [
    "p = [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99]\n",
    "dy_percentiles = da.percentile(dy, p).compute()\n",
    "\n",
    "for i, percentile in enumerate(p):\n",
    "    print(f\"{percentile * 100}%: {round(dy_percentiles[i], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-clone",
   "metadata": {},
   "source": [
    "The metrics functions from `dask-ml` match those from `scikit-learn`, but take in and return Dask collections. You can use these functions to perform model evaluation without the evaluation data or predictions needing to be pulled down to the machine running this notebook. Pretty cool, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "considered-holocaust",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.868643310017465"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_ml.metrics.regression import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(preds, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-teddy",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Learn more: https://lightgbm.readthedocs.io/en/latest/Python-API.html#dask-api.\n",
    "\n",
    "Ask a question, report a bug, or submit a feature request: https://github.com/microsoft/LightGBM/issues.\n",
    "\n",
    "Contribute: https://github.com/microsoft/LightGBM/issues?q=is%3Aissue+is%3Aopen+label%3Adask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
