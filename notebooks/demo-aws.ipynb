{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tribal-xerox",
   "metadata": {},
   "source": [
    "# LightGBM + Dask\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"./_img/lightgbm.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./_img/dask-horizontal.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./_img/aws.svg\" width=\"150\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "This notebook shows how to use `lightgbm.dask` to train a LightGBM model on data stored as a [Dask Array](https://docs.dask.org/en/latest/array.html). It uses `FargateCluster` from [`dask-cloudprovider`](https://github.com/dask/dask-cloudprovider) to create a distributed cluster running on [AWS Fargate](https://aws.amazon.com/fargate/).\n",
    "\n",
    "To explore other topics in greater depth, see the other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-hunger",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Set up a Dask cluster on AWS Fargate\n",
    "\n",
    "Before running any of the code in the notebook, follow the instructions in [\"Test with a FargateCluster\"](../README.md##test-with-a-fargatecluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expanded-declaration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduler and worker image: public.ecr.aws/w8s1c8b1/lightgbm-dask-testing-cluster-jlamb:py3.12-dask2024.6.2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"../ecr-details.json\", \"r\") as f:\n",
    "    ecr_details = json.loads(f.read())\n",
    "\n",
    "IMAGE_REPO = ecr_details[\"repository\"][\"repositoryUri\"]\n",
    "IMAGE_TAG = os.environ[\"IMAGE_TAG\"]\n",
    "IMAGE_URI = f\"{IMAGE_REPO}:{IMAGE_TAG}\"\n",
    "print(f\"scheduler and worker image: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-bosnia",
   "metadata": {},
   "source": [
    "Before proceeding, set up your AWS credentials. If you're unsure how to do this, see [the AWS docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html).\n",
    "\n",
    "Next, determine the CPU architecture of the machine you're running on.\n",
    "This project builds single-architecture container images matching the host system, so it's important\n",
    "to use the same CPU architecture on AWS Fargate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a10d61c-5251-46a7-9f16-bd6eef606a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.machine().lower() in {\"aarch64\", \"arm64\"}:\n",
    "    cpu_architecture=\"ARM64\"\n",
    "else:\n",
    "    cpu_architecture=\"X86_64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-little",
   "metadata": {},
   "source": [
    "Create a cluster with 3 workers. See https://cloudprovider.dask.org/en/latest/aws.html#dask_cloudprovider.aws.FargateCluster for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-collect",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/contextlib.py:144: UserWarning: Creating your cluster is taking a surprisingly long time. This is likely due to pending resources on AWS. Hang tight! \n",
      "  next(self.gen)\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cloudprovider.aws import FargateCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = FargateCluster(\n",
    "    image=IMAGE_URI,\n",
    "    cpu_architecture=cpu_architecture,\n",
    "    worker_cpu=512,\n",
    "    worker_mem=4096,\n",
    "    n_workers=n_workers,\n",
    "    fargate_use_private_ip=False,\n",
    "    scheduler_timeout=\"40 minutes\",\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "raising-mauritius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the dashboard: http://52.55.229.38:8787/status\n"
     ]
    }
   ],
   "source": [
    "print(f\"View the dashboard: {cluster.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-composition",
   "metadata": {},
   "source": [
    "Click the link above to view a diagnostic dashboard while you run the training code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-lincoln",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Get some training data\n",
    "\n",
    "This example uses `sklearn.datasets.make_regression()` to generate a dataset in `numpy` format, then uses `dask.Array.from_array()` to turn that into a Dask Array.\n",
    "\n",
    "That's just done for convenience. `lightgbm.dask` just expects that your data are Dask Arrays or Dask DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "structural-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=10000, random_state=42)\n",
    "dX = da.from_array(X, chunks=(1000, X.shape[1]))\n",
    "dy = da.from_array(y, chunks=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-future",
   "metadata": {},
   "source": [
    "Right now, the Dask Arrays `data` and `labels` are lazy. Before training, you can force the cluster to compute them by running `.persist()` and then wait for that computation to finish by `wait()`-ing on them.\n",
    "\n",
    "Doing this is optional, but it will make data loading a one-time cost so subsequent runs are fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "quiet-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "\n",
    "dX = dX.persist()\n",
    "dy = dy.persist()\n",
    "_ = wait([dX, dy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-corner",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Train a model\n",
    "\n",
    "With the data set up on the workers, train a model. `lightgbm.dask.DaskLGBMRegressor` has an interface that tries to stay as close as possible to the non-Dask scikit-learn interface to LightGBM (`lightgbm.sklearn.LGBMRegressor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pleased-brunei",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DaskLGBMRegressor\n\u001b[1;32m      3\u001b[0m dask_reg \u001b[38;5;241m=\u001b[39m DaskLGBMRegressor(\n\u001b[1;32m      4\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m      5\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     min_child_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m dask_reg\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     14\u001b[0m     X\u001b[38;5;241m=\u001b[39mdX,\n\u001b[1;32m     15\u001b[0m     y\u001b[38;5;241m=\u001b[39mdy,\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "from lightgbm.dask import DaskLGBMRegressor\n",
    "\n",
    "dask_reg = DaskLGBMRegressor(\n",
    "    client=client,\n",
    "    max_depth=5,\n",
    "    objective=\"regression_l1\",\n",
    "    learning_rate=0.1,\n",
    "    tree_learner=\"data\",\n",
    "    n_estimators=100,\n",
    "    min_child_samples=1,\n",
    ")\n",
    "\n",
    "dask_reg.fit(\n",
    "    X=dX,\n",
    "    y=dy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-kidney",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Evaluate the model\n",
    "\n",
    "The `.predict()` method takes in a Dask collection and returns a Dask Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dask_reg.predict(dX)\n",
    "print(str(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-trademark",
   "metadata": {},
   "source": [
    "Before calculating the mean absolute error (MAE) of these predictions, compute some summary statistics on the target variable. This is necessary to understand what \"good\" values of MAE look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99]\n",
    "dy_percentiles = da.percentile(dy, p).compute()\n",
    "\n",
    "for i, percentile in enumerate(p):\n",
    "    print(f\"{percentile * 100}%: {round(dy_percentiles[i], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-switch",
   "metadata": {},
   "source": [
    "The metrics functions from `dask-ml` match those from `scikit-learn`, but take in and return Dask collections. You can use these functions to perform model evaluation without the evaluation data or predictions needing to be pulled down to the machine running this notebook. Pretty cool, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.metrics.regression import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(preds, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-region",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Learn more: https://lightgbm.readthedocs.io/en/latest/Python-API.html#dask-api.\n",
    "\n",
    "Ask a question, report a bug, or submit a feature request: https://github.com/microsoft/LightGBM/issues.\n",
    "\n",
    "Contribute: https://github.com/microsoft/LightGBM/issues?q=is%3Aissue+is%3Aopen+label%3Adask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
