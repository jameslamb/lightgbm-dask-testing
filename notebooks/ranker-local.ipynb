{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask LightGBMRanker\n",
    "\n",
    "This notebook contains tests on `lightgbm.dask.LGBMRanker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.datasets import make_blobs, make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the dashboard: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "n_workers = 3\n",
    "cluster = LocalCluster(n_workers=n_workers)\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(n_workers)\n",
    "\n",
    "print(f\"View the dashboard: {cluster.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "\n",
    "def make_ranking(\n",
    "    n_samples=100,\n",
    "    n_features=20,\n",
    "    n_informative=5,\n",
    "    gmax=2,\n",
    "    group=None,\n",
    "    random_gs=False,\n",
    "    avg_gs=10,\n",
    "    random_state=0,\n",
    "):\n",
    "    rnd_generator = check_random_state(random_state)\n",
    "\n",
    "    y_vec, group_id_vec = np.empty((0,), dtype=int), np.empty((0,), dtype=int)\n",
    "    gid = 0\n",
    "\n",
    "    # build target, group ID vectors.\n",
    "    relvalues = range(gmax + 1)\n",
    "\n",
    "    # build y/target and group-id vectors with user-specified group sizes.\n",
    "    if group is not None and hasattr(group, \"__len__\"):\n",
    "        n_samples = np.sum(group)\n",
    "\n",
    "        for i, gsize in enumerate(group):\n",
    "            y_vec = np.concatenate(\n",
    "                (y_vec, rnd_generator.choice(relvalues, size=gsize, replace=True))\n",
    "            )\n",
    "            group_id_vec = np.concatenate((group_id_vec, [i] * gsize))\n",
    "\n",
    "    # build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.\n",
    "    else:\n",
    "        while len(y_vec) < n_samples:\n",
    "            gsize = avg_gs if not random_gs else rnd_generator.poisson(avg_gs)\n",
    "\n",
    "            # groups should contain > 1 element for pairwise learning objective.\n",
    "            if gsize < 1:\n",
    "                continue\n",
    "\n",
    "            y_vec = np.append(y_vec, rnd_generator.choice(relvalues, size=gsize, replace=True))\n",
    "            group_id_vec = np.append(group_id_vec, [gid] * gsize)\n",
    "            gid += 1\n",
    "\n",
    "        y_vec, group_id_vec = y_vec[:n_samples], group_id_vec[:n_samples]\n",
    "\n",
    "    # build feature data, X. Transform first few into informative features.\n",
    "    n_informative = max(min(n_features, n_informative), 0)\n",
    "    X = rnd_generator.uniform(size=(n_samples, n_features))\n",
    "\n",
    "    for j in range(n_informative):\n",
    "        bias, coef = rnd_generator.normal(size=2)\n",
    "        X[:, j] = bias + coef * y_vec\n",
    "\n",
    "    return X, y_vec, group_id_vec\n",
    "\n",
    "\n",
    "def _create_ranking_data(n_samples=100, chunk_size=50, **kwargs):\n",
    "    X, y, g = make_ranking(n_samples=n_samples, random_state=42, **kwargs)\n",
    "    rnd = np.random.RandomState(42)\n",
    "    w = rnd.rand(X.shape[0]) * 0.01\n",
    "    g_rle = np.array([len(list(grp)) for _, grp in groupby(g)])\n",
    "\n",
    "    # ranking arrays: one chunk per group. Each chunk must include all columns.\n",
    "    p = X.shape[1]\n",
    "    dX, dy, dw, dg = [], [], [], []\n",
    "    for g_idx, rhs in enumerate(np.cumsum(g_rle)):\n",
    "        lhs = rhs - g_rle[g_idx]\n",
    "        dX.append(da.from_array(X[lhs:rhs, :], chunks=(rhs - lhs, p)))\n",
    "        dy.append(da.from_array(y[lhs:rhs]))\n",
    "        dw.append(da.from_array(w[lhs:rhs]))\n",
    "        dg.append(da.from_array(np.array([g_rle[g_idx]])))\n",
    "\n",
    "    dX = da.concatenate(dX, axis=0)\n",
    "    dy = da.concatenate(dy, axis=0)\n",
    "    dw = da.concatenate(dw, axis=0)\n",
    "    dg = da.concatenate(dg, axis=0)\n",
    "\n",
    "    return X, y, w, g_rle, dX, dy, dw, dg\n",
    "\n",
    "\n",
    "def _create_data(objective, n_samples=100, centers=2, chunk_size=50):\n",
    "    if objective == \"classification\":\n",
    "        X, y = make_blobs(n_samples=n_samples, centers=centers, random_state=42)\n",
    "    elif objective == \"regression\":\n",
    "        X, y = make_regression(n_samples=n_samples, random_state=42)\n",
    "    rnd = np.random.RandomState(42)\n",
    "    weights = rnd.random(X.shape[0]) * 0.01\n",
    "    dX = da.from_array(X, (chunk_size, X.shape[1]))\n",
    "    dy = da.from_array(y, chunk_size)\n",
    "    dw = da.from_array(weights, chunk_size)\n",
    "    return X, y, weights, dX, dy, dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"random_state\": 42, \"n_estimators\": 50, \"num_leaves\": 20, \"min_child_samples\": 1}\n",
    "\n",
    "dask_ranker = lgb.DaskLGBMRanker(\n",
    "    client=client, time_out=5, local_listen_port=12400, tree_learner_type=\"data_parallel\", **params\n",
    ")\n",
    "local_ranker = lgb.LGBMRanker(**params)\n",
    "\n",
    "dask_regressor = lgb.DaskLGBMRegressor(\n",
    "    client=client, time_out=5, local_listen_port=12400, tree_learner_type=\"data_parallel\", **params\n",
    ")\n",
    "local_regressor = lgb.LGBMRegressor(**params)\n",
    "\n",
    "dask_classifier = lgb.DaskLGBMClassifier(\n",
    "    client=client, time_out=5, local_listen_port=12400, tree_learner_type=\"data_parallel\", **params\n",
    ")\n",
    "local_classifier = lgb.LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- ranking ----- #\n",
    "X, y, w, g, dX, dy, dw, dg = _create_ranking_data()\n",
    "\n",
    "dask_summaries = {}\n",
    "local_summaries = {}\n",
    "for _ in range(100):\n",
    "    dask_ranker = dask_ranker.fit(dX, dy, sample_weight=dw, group=dg)\n",
    "    num_trees = dask_ranker.booster_.num_trees()\n",
    "    dask_summaries[num_trees] = dask_summaries.get(num_trees, 0) + 1\n",
    "\n",
    "    local_ranker.fit(X, y, sample_weight=w, group=g)\n",
    "    num_trees = local_ranker.booster_.num_trees()\n",
    "    local_summaries[num_trees] = local_summaries.get(num_trees, 0) + 1\n",
    "\n",
    "print(\"   dask: \" + str(dask_summaries))\n",
    "print(\"sklearn: \" + str(local_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dask: {50: 100}\n",
      "sklearn: {50: 100}\n"
     ]
    }
   ],
   "source": [
    "# ----- regression ----- #\n",
    "X, y, w, dX, dy, dw = _create_data(objective=\"regression\")\n",
    "\n",
    "dask_summaries = {}\n",
    "local_summaries = {}\n",
    "for _ in range(100):\n",
    "    dask_regressor.fit(dX, dy, sample_weight=dw)\n",
    "    num_trees = dask_regressor.booster_.num_trees()\n",
    "    dask_summaries[num_trees] = dask_summaries.get(num_trees, 0) + 1\n",
    "\n",
    "    local_regressor.fit(X, y, sample_weight=w)\n",
    "    num_trees = local_regressor.booster_.num_trees()\n",
    "    local_summaries[num_trees] = local_summaries.get(num_trees, 0) + 1\n",
    "\n",
    "print(\"   dask: \" + str(dask_summaries))\n",
    "print(\"sklearn: \" + str(local_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dask: {47: 53, 46: 47}\n",
      "sklearn: {48: 100}\n"
     ]
    }
   ],
   "source": [
    "# ----- binary classification ----- #\n",
    "X, y, w, dX, dy, dw = _create_data(objective=\"classification\")\n",
    "\n",
    "dask_summaries = {}\n",
    "local_summaries = {}\n",
    "for _ in range(100):\n",
    "    dask_classifier.fit(dX, dy, sample_weight=dw)\n",
    "    num_trees = dask_classifier.booster_.num_trees()\n",
    "    dask_summaries[num_trees] = dask_summaries.get(num_trees, 0) + 1\n",
    "\n",
    "    local_classifier.fit(X, y, sample_weight=w)\n",
    "    num_trees = local_classifier.booster_.num_trees()\n",
    "    local_summaries[num_trees] = local_summaries.get(num_trees, 0) + 1\n",
    "\n",
    "print(\"   dask: \" + str(dask_summaries))\n",
    "print(\"sklearn: \" + str(local_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dask: {117: 54, 111: 46}\n",
      "sklearn: {150: 100}\n"
     ]
    }
   ],
   "source": [
    "# ----- multiclass classification ----- #\n",
    "dask_classifier = lgb.DaskLGBMClassifier(\n",
    "    client=client, time_out=5, local_listen_port=12400, tree_learner_type=\"data_parallel\", **params\n",
    ")\n",
    "local_classifier = lgb.LGBMClassifier(**params)\n",
    "\n",
    "X, y, w, dX, dy, dw = _create_data(objective=\"classification\", centers=3)\n",
    "\n",
    "dask_summaries = {}\n",
    "local_summaries = {}\n",
    "for _ in range(100):\n",
    "    dask_classifier.fit(dX, dy, sample_weight=dw)\n",
    "    num_trees = dask_classifier.booster_.num_trees()\n",
    "    dask_summaries[num_trees] = dask_summaries.get(num_trees, 0) + 1\n",
    "\n",
    "    local_classifier.fit(X, y, sample_weight=w)\n",
    "    num_trees = local_classifier.booster_.num_trees()\n",
    "    local_summaries[num_trees] = local_summaries.get(num_trees, 0) + 1\n",
    "\n",
    "print(\"   dask: \" + str(dask_summaries))\n",
    "print(\"sklearn: \" + str(local_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
