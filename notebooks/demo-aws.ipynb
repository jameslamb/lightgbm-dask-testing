{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classified-celtic",
   "metadata": {},
   "source": [
    "# LightGBM + Dask\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"./_img/lightgbm.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./_img/dask-horizontal.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./_img/aws.svg\" width=\"150\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "This notebook shows how to use `lightgbm.dask` to train a LightGBM model on data stored as a [Dask Array](https://docs.dask.org/en/latest/array.html). It uses `FargateCluster` from [`dask-cloudprovider`](https://github.com/dask/dask-cloudprovider) to create a distributed cluster running on [AWS Fargate](https://aws.amazon.com/fargate/).\n",
    "\n",
    "To explore other topics in greater depth, see the other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-little",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Set up a Dask cluster on AWS Fargate\n",
    "\n",
    "Before running any of the code in the notebook, follow the instructions in [\"Test with a FargateCluster\"](../README.md##test-with-a-fargatecluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../ecr-details.json\", \"r\") as f:\n",
    "    ecr_details = json.loads(f.read())\n",
    "\n",
    "CONTAINER_IMAGE = ecr_details[\"repository\"][\"repositoryUri\"] + \":1\"\n",
    "print(f\"scheduler and worker image: {CONTAINER_IMAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-handle",
   "metadata": {},
   "source": [
    "Before proceeding, set up your AWS credentials. If you're unsure how to do this, see [the AWS docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-school",
   "metadata": {},
   "source": [
    "Create a cluster with 3 workers. See https://cloudprovider.dask.org/en/latest/aws.html#dask_cloudprovider.aws.FargateCluster for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cloudprovider.aws import FargateCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = FargateCluster(\n",
    "    image=CONTAINER_IMAGE,\n",
    "    worker_cpu=512,\n",
    "    worker_mem=4096,\n",
    "    n_workers=n_workers,\n",
    "    fargate_use_private_ip=False,\n",
    "    scheduler_timeout=\"40 minutes\",\n",
    "    find_address_timeout=60 * 10,\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"View the dashboard: {cluster.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-ireland",
   "metadata": {},
   "source": [
    "Click the link above to view a diagnostic dashboard while you run the training code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-estate",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Get some training data\n",
    "\n",
    "This example uses `sklearn.datasets.make_regression()` to generate a dataset in `numpy` format, then uses `dask.Array.from_array()` to turn that into a Dask Array.\n",
    "\n",
    "That's just done for convenience. `lightgbm.dask` just expects that your data are Dask Arrays or Dask DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=10000, random_state=42)\n",
    "dX = da.from_array(X, chunks=(1000, X.shape[1]))\n",
    "dy = da.from_array(y, chunks=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-palestine",
   "metadata": {},
   "source": [
    "Right now, the Dask Arrays `data` and `labels` are lazy. Before training, you can force the cluster to compute them by running `.persist()` and then wait for that computation to finish by `wait()`-ing on them.\n",
    "\n",
    "Doing this is optional, but it will make data loading a one-time cost so subsequent runs are fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "\n",
    "dX = dX.persist()\n",
    "dy = dy.persist()\n",
    "_ = wait([dX, dy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-leone",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Train a model\n",
    "\n",
    "With the data set up on the workers, train a model. `lightgbm.dask.DaskLGBMRegressor` has an interface that tries to stay as close as possible to the non-Dask scikit-learn interface to LightGBM (`lightgbm.sklearn.LGBMRegressor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.dask import DaskLGBMRegressor\n",
    "\n",
    "dask_reg = DaskLGBMRegressor(\n",
    "    client=client,\n",
    "    max_depth=5,\n",
    "    objective=\"regression_l1\",\n",
    "    learning_rate=0.1,\n",
    "    tree_learner=\"data\",\n",
    "    n_estimators=10,\n",
    "    min_child_samples=1,\n",
    ")\n",
    "\n",
    "dask_reg.fit(\n",
    "    X=dX,\n",
    "    y=dy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-princess",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Evaluate the model\n",
    "\n",
    "The `.predict()` method takes in a Dask collection and returns a Dask Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dask_reg.predict(dX)\n",
    "print(str(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-dairy",
   "metadata": {},
   "source": [
    "Before calculating the mean absolute error (MAE) of these predictions, compute some summary statistics on the target variable. This is necessary to understand what \"good\" values of MAE look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99]\n",
    "dy_percentiles = da.percentile(dy, p).compute()\n",
    "\n",
    "for i, percentile in enumerate(p):\n",
    "    print(f\"{percentile * 100}%: {dy_percentiles[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-donna",
   "metadata": {},
   "source": [
    "The metrics functions from `dask-ml` match those from `scikit-learn`, but take in and return Dask collections. You can use these functions to perform model evaluation without the evaluation data or predictions needing to be pulled down to the machine running this notebook. Pretty cool, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.metrics.regression import mean_absolute_error\n",
    "mean_absolute_error(preds, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-institution",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Learn more: https://lightgbm.readthedocs.io/en/latest/Python-API.html#dask-api.\n",
    "\n",
    "Ask a question, report a bug, or submit a feature request: https://github.com/microsoft/LightGBM/issues.\n",
    "\n",
    "Contribute: https://github.com/microsoft/LightGBM/issues?q=is%3Aissue+is%3Aopen+label%3Adask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
